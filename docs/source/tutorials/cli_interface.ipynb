{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CLI Interface\n",
        "This notebook illustrates the basic usage of the vis4d CLI interface."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Faster RCNN on COCO\n",
        "The following bash commands trains a Faster RCNN model on a subset of the COCO dataset using the CPU for one epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/zrene/anaconda3/envs/vis4d/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/home/zrene/anaconda3/envs/vis4d/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Starting training\n",
            "Environment info: PyTorch version: 1.13.1+cu117\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 11.7\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 20.04.4 LTS (x86_64)\n",
            "GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "Clang version: Could not collect\n",
            "CMake version: version 3.24.0-rc5\n",
            "Libc version: glibc-2.31\n",
            "\n",
            "Python version: 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:26:04) [GCC 10.4.0] (64-bit runtime)\n",
            "Python platform: Linux-5.4.0-139-generic-x86_64-with-glibc2.31\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: Could not collect\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080 Laptop GPU\n",
            "Nvidia driver version: 515.86.01\n",
            "cuDNN version: Could not collect\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] mypy==0.991\n",
            "[pip3] mypy-extensions==0.4.3\n",
            "[pip3] numpy==1.23.0\n",
            "[pip3] pytorch-lightning==1.7.0\n",
            "[pip3] ros_numpy==0.0.5\n",
            "[pip3] torch==1.13.1+cu117\n",
            "[pip3] torch-scatter==2.1.0\n",
            "[pip3] torchmetrics==0.11.0\n",
            "[pip3] torchvision==0.14.1+cu117\n",
            "[conda] Could not collect\n",
            "\n",
            " Using 0/1 GPUs\n",
            "Load checkpoint from http path: https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\n",
            "Loading <bound method COCO.__repr__ of COCODataset(root=/home/zrene/git/vis4d/tests/vis4d-test-data/coco_test, split=train, use_pascal_voc_cats=False)> takes 0.00 seconds.\n",
            "Loading <bound method COCO.__repr__ of COCODataset(root=/home/zrene/git/vis4d/tests/vis4d-test-data/coco_test, split=train, use_pascal_voc_cats=False)> takes 0.00 seconds.\n",
            "Epoch 1: 0/1, ETA: 0:00:00, 13.44s/it, loss: 1.044, __RPNLoss.loss_cls: 0.138, __RPNLoss.loss_bbox: 0.094, __RCNNLoss.rcnn_loss_cls: 0.432, __RCNNLoss.rcnn_loss_bbox: 0.380\n",
            "Running validation...\n",
            "100%|█████████████████████████████████████████████| 2/2 [00:04<00:00,  2.29s/it]\n",
            "Running evaluator <vis4d.eval.detect.coco.COCOEvaluator object at 0x7f532a01e260>...\n",
            "AP: 0.466\n",
            "AP50: 0.636\n",
            "AP75: 0.542\n",
            "APs: 0.436\n",
            "APm: 0.674\n",
            "APl: 0.550\n",
            "Showing results for COCO_AP\n",
            "\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.466\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.636\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.542\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.436\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.674\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.550\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.304\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.557\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.557\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.526\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.686\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.567\n",
            "\n",
            "Training done.\n"
          ]
        }
      ],
      "source": [
        "!python -m vis4d.engine.run --config ../../../vis4d/config/example/faster_rcnn_coco.py --gpus 0 --config.num_epochs 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If needed, you can also print the full configuration of the training run by passing the `--print_config` flag.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/zrene/anaconda3/envs/vis4d/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/home/zrene/anaconda3/envs/vis4d/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Starting training\n",
            "Environment info: PyTorch version: 1.13.1+cu117\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 11.7\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 20.04.4 LTS (x86_64)\n",
            "GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "Clang version: Could not collect\n",
            "CMake version: version 3.24.0-rc5\n",
            "Libc version: glibc-2.31\n",
            "\n",
            "Python version: 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:26:04) [GCC 10.4.0] (64-bit runtime)\n",
            "Python platform: Linux-5.4.0-139-generic-x86_64-with-glibc2.31\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: Could not collect\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080 Laptop GPU\n",
            "Nvidia driver version: 515.86.01\n",
            "cuDNN version: Could not collect\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] mypy==0.991\n",
            "[pip3] mypy-extensions==0.4.3\n",
            "[pip3] numpy==1.23.0\n",
            "[pip3] pytorch-lightning==1.7.0\n",
            "[pip3] ros_numpy==0.0.5\n",
            "[pip3] torch==1.13.1+cu117\n",
            "[pip3] torch-scatter==2.1.0\n",
            "[pip3] torchmetrics==0.11.0\n",
            "[pip3] torchvision==0.14.1+cu117\n",
            "[conda] Could not collect\n",
            "********************************************************************************\n",
            "\n",
            "data_connector: \n",
            " class_path: vis4d.engine.connectors.StaticDataConnector\n",
            " init_args: \n",
            "  connections: \n",
            "   callbacks: \n",
            "    bbox_vis_test: \n",
            "     boxes: \n",
            "      key: boxes\n",
            "      source: prediction\n",
            "     images: \n",
            "      key: images\n",
            "      source: data\n",
            "    coco_eval_test: \n",
            "     coco_image_id: \n",
            "      key: coco_image_id\n",
            "      source: data\n",
            "     pred_boxes: \n",
            "      key: boxes\n",
            "      source: prediction\n",
            "     pred_classes: \n",
            "      key: class_ids\n",
            "      source: prediction\n",
            "     pred_scores: \n",
            "      key: scores\n",
            "      source: prediction\n",
            "   loss: \n",
            "    boxes: \n",
            "     key: sampled_proposals.boxes\n",
            "     source: prediction\n",
            "    boxes_mask: \n",
            "     key: sampled_targets.labels\n",
            "     source: prediction\n",
            "    class_outs: \n",
            "     key: roi.cls_score\n",
            "     source: prediction\n",
            "    cls_outs: \n",
            "     key: rpn.cls\n",
            "     source: prediction\n",
            "    images_hw: \n",
            "     key: input_hw\n",
            "     source: data\n",
            "    pred_sampled_proposals: \n",
            "     key: sampled_proposals\n",
            "     source: prediction\n",
            "    reg_outs: \n",
            "     key: rpn.box\n",
            "     source: prediction\n",
            "    regression_outs: \n",
            "     key: roi.bbox_pred\n",
            "     source: prediction\n",
            "    target_boxes: \n",
            "     key: sampled_targets.boxes\n",
            "     source: prediction\n",
            "    target_classes: \n",
            "     key: sampled_targets.classes\n",
            "     source: prediction\n",
            "   test: \n",
            "    boxes2d: boxes2d\n",
            "    boxes2d_classes: boxes2d_classes\n",
            "    images: images\n",
            "    input_hw: input_hw\n",
            "    original_hw: original_hw\n",
            "   train: \n",
            "    boxes2d: boxes2d\n",
            "    boxes2d_classes: boxes2d_classes\n",
            "    images: images\n",
            "    input_hw: input_hw\n",
            "dataset_root: /home/zrene/git/vis4d/tests/vis4d-test-data/coco_test\n",
            "experiment_name: frcnn_coco\n",
            "gen: \n",
            "loss: \n",
            " class_path: vis4d.config.default.loss.faster_rcnn_loss.get_default_faster_rcnn_loss\n",
            " init_args: \n",
            "  anchor_generator: \n",
            "   class_path: vis4d.op.detect.faster_rcnn.get_default_anchor_generator\n",
            "  rcnn_box_encoder: \n",
            "   class_path: vis4d.op.detect.faster_rcnn.get_default_rcnn_box_encoder\n",
            "  rpn_box_encoder: \n",
            "   class_path: vis4d.op.detect.faster_rcnn.get_default_rpn_box_encoder\n",
            "model: \n",
            " class_path: vis4d.model.detect.faster_rcnn.FasterRCNN\n",
            " init_args: \n",
            "  anchor_generator: \n",
            "   class_path: vis4d.op.detect.faster_rcnn.get_default_anchor_generator\n",
            "  num_classes: 80\n",
            "  rcnn_box_encoder: \n",
            "   class_path: vis4d.op.detect.faster_rcnn.get_default_rcnn_box_encoder\n",
            "  rpn_box_encoder: \n",
            "   class_path: vis4d.op.detect.faster_rcnn.get_default_rpn_box_encoder\n",
            "  weights: mmdet\n",
            "n_gpus: 0\n",
            "num_epochs: 1\n",
            "optimizers: \n",
            " - class_path: vis4d.engine.opt.Optimizer\n",
            "   init_args: \n",
            "    lr_scheduler_cb: \n",
            "     class_path: vis4d.config.util.DelayedInstantiator\n",
            "     init_args: \n",
            "      instantiable: \n",
            "       _class_path: torch.optim.lr_scheduler.StepLR\n",
            "       init_args: \n",
            "        gamma: 0.1\n",
            "        step_size: 3\n",
            "    lr_warmup: None\n",
            "    optimizer_cb: \n",
            "     class_path: vis4d.config.util.DelayedInstantiator\n",
            "     init_args: \n",
            "      instantiable: \n",
            "       _class_path: torch.optim.sgd.SGD\n",
            "       init_args: \n",
            "        lr: 0.01\n",
            " \n",
            "params: \n",
            " augment_proba: 0.5\n",
            " batch_size: 16\n",
            " lr: 0.01\n",
            " num_classes: 80\n",
            "save_prefix: vis4d-workspace/test/frcnn_coco\n",
            "test_callbacks: \n",
            " bbox_vis: \n",
            "  class_path: vis4d.common.callbacks.VisualizerCallback\n",
            "  init_args: \n",
            "   num_epochs: 1\n",
            "   output_dir: vis4d-workspace/test/frcnn_coco/vis\n",
            "   run_every_nth_epoch: 1\n",
            "   visualizer: \n",
            "    class_path: vis4d.vis.image.bounding_box_visualizer.BoundingBoxVisualizer\n",
            " coco_eval: \n",
            "  class_path: vis4d.common.callbacks.EvaluatorCallback\n",
            "  init_args: \n",
            "   evaluator: \n",
            "    class_path: vis4d.eval.detect.coco.COCOEvaluator\n",
            "    init_args: \n",
            "     data_root: /home/zrene/git/vis4d/tests/vis4d-test-data/coco_test\n",
            "     split: train\n",
            "   num_epochs: 1\n",
            "   run_every_nth_epoch: 1\n",
            "test_dl: \n",
            " coco_eval: \n",
            "  class_path: vis4d.data.loader.build_train_dataloader\n",
            "  init_args: \n",
            "   batchprocess_fn: \n",
            "    class_path: vis4d.data.transforms.pad.pad_image\n",
            "   dataset: \n",
            "    class_path: vis4d.data.loader.DataPipe\n",
            "    init_args: \n",
            "     datasets: \n",
            "      class_path: vis4d.data.datasets.coco.COCO\n",
            "      init_args: \n",
            "       data_root: /home/zrene/git/vis4d/tests/vis4d-test-data/coco_test\n",
            "       keys: \n",
            "        - images\n",
            "        - boxes2d\n",
            "        - boxes2d_classes\n",
            " \n",
            "       split: train\n",
            "     preprocess_fn: \n",
            "      class_path: vis4d.data.transforms.base.compose\n",
            "      init_args: \n",
            "       transforms: \n",
            "        - class_path: vis4d.data.transforms.resize.resize_image\n",
            "          init_args: \n",
            "           keep_ratio: True\n",
            "           shape: \n",
            "            - 800\n",
            "            - 1333\n",
            " \n",
            "        - class_path: vis4d.data.transforms.resize.resize_boxes2d\n",
            "        - class_path: vis4d.data.transforms.base.random_apply\n",
            "          init_args: \n",
            "           probability: 0\n",
            "           transforms: \n",
            "            - class_path: vis4d.data.transforms.flip.flip_image\n",
            "            - class_path: vis4d.data.transforms.flip.flip_boxes2d\n",
            " \n",
            "        - class_path: vis4d.data.transforms.normalize.normalize_image\n",
            " \n",
            "   samples_per_gpu: 1\n",
            "   shuffle: False\n",
            "   workers_per_gpu: 1\n",
            "test_split: train\n",
            "train_callbacks: \n",
            " ckpt: \n",
            "  class_path: vis4d.common.callbacks.CheckpointCallback\n",
            "  init_args: \n",
            "   num_epochs: 1\n",
            "   run_every_nth_epoch: 1\n",
            "   save_prefix: vis4d-workspace/test/frcnn_coco\n",
            " logging: \n",
            "  class_path: vis4d.common.callbacks.LoggingCallback\n",
            "  init_args: \n",
            "   refresh_rate: 1\n",
            "train_dl: \n",
            " class_path: vis4d.data.loader.build_train_dataloader\n",
            " init_args: \n",
            "  batchprocess_fn: \n",
            "   class_path: vis4d.data.transforms.pad.pad_image\n",
            "  dataset: \n",
            "   class_path: vis4d.data.loader.DataPipe\n",
            "   init_args: \n",
            "    datasets: \n",
            "     class_path: vis4d.data.datasets.coco.COCO\n",
            "     init_args: \n",
            "      data_root: /home/zrene/git/vis4d/tests/vis4d-test-data/coco_test\n",
            "      keys: \n",
            "       - images\n",
            "       - boxes2d\n",
            "       - boxes2d_classes\n",
            " \n",
            "      split: train\n",
            "    preprocess_fn: \n",
            "     class_path: vis4d.data.transforms.base.compose\n",
            "     init_args: \n",
            "      transforms: \n",
            "       - class_path: vis4d.data.transforms.resize.resize_image\n",
            "         init_args: \n",
            "          keep_ratio: True\n",
            "          shape: \n",
            "           - 800\n",
            "           - 1333\n",
            " \n",
            "       - class_path: vis4d.data.transforms.resize.resize_boxes2d\n",
            "       - class_path: vis4d.data.transforms.base.random_apply\n",
            "         init_args: \n",
            "          probability: 0.5\n",
            "          transforms: \n",
            "           - class_path: vis4d.data.transforms.flip.flip_image\n",
            "           - class_path: vis4d.data.transforms.flip.flip_boxes2d\n",
            " \n",
            "       - class_path: vis4d.data.transforms.normalize.normalize_image\n",
            " \n",
            "  samples_per_gpu: 16\n",
            "  shuffle: True\n",
            "  workers_per_gpu: 4\n",
            "train_split: train\n",
            "\n",
            "********************************************************************************\n",
            "\n",
            " Using 0/1 GPUs\n",
            "Load checkpoint from http path: https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\n",
            "Loading <bound method COCO.__repr__ of COCODataset(root=/home/zrene/git/vis4d/tests/vis4d-test-data/coco_test, split=train, use_pascal_voc_cats=False)> takes 0.00 seconds.\n",
            "Loading <bound method COCO.__repr__ of COCODataset(root=/home/zrene/git/vis4d/tests/vis4d-test-data/coco_test, split=train, use_pascal_voc_cats=False)> takes 0.00 seconds.\n",
            "Epoch 1: 0/1, ETA: 0:00:00, 13.82s/it, loss: 1.034, __RPNLoss.loss_cls: 0.132, __RPNLoss.loss_bbox: 0.095, __RCNNLoss.rcnn_loss_cls: 0.424, __RCNNLoss.rcnn_loss_bbox: 0.383\n",
            "Running validation...\n",
            "100%|█████████████████████████████████████████████| 2/2 [00:04<00:00,  2.43s/it]\n",
            "Running evaluator <vis4d.eval.detect.coco.COCOEvaluator object at 0x7f2425c8e260>...\n",
            "AP: 0.514\n",
            "AP50: 0.643\n",
            "AP75: 0.602\n",
            "APs: 0.477\n",
            "APm: 0.764\n",
            "APl: 0.633\n",
            "Showing results for COCO_AP\n",
            "\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.514\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.643\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.602\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.477\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.764\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.633\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.328\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.638\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.638\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.594\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.764\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.633\n",
            "\n",
            "Training done.\n"
          ]
        }
      ],
      "source": [
        "!python -m vis4d.engine.run --config ../../../vis4d/config/example/faster_rcnn_coco.py --gpus 0 --config.num_epochs 1 --print-config"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.8 ('vis4d')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "8a7b97dc50f087d28fa11186d932973d2d4b06a79ca606b84277a179bb90937d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
